{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Notebook Header Start -->\n",
    "\n",
    "<h1 align=\"center\">Changes to Noxious Stimuli by means of Dorsal Root Ganglion Stimulation</h1>\n",
    "\n",
    "<p align=\"center\">\n",
    "  <strong>Author:</strong> Karl Bates<br>\n",
    "  <strong>Date:</strong> 2025-04-26<br>\n",
    "  <strong>Affiliation:</strong> Carnegie Mellon University, Cohen-Karni Lab  || Neuromechatronics Lab\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## üìä Notebook Outline\n",
    "\n",
    "* **Importing libraries & data**\n",
    "* **Preprocess neurophysiology recordings for spike sorting**\n",
    "* **Package preprocessed data for spike sorting using Kilosort4**\n",
    "* **Run Kilosort to extract spike activity**\n",
    "* **Calculate average firing rate of each cluster during noxious stimuli**\n",
    "* **Compare the firing rates of clusters before and after noxious stimuli**\n",
    "\n",
    "## üìö References & Additional Resources\n",
    "\n",
    "- [Kilosort4 docs](https://github.com/MouseLand/Kilosort/tree/main)\n",
    "- [SpikeInterface docs](https://github.com/SpikeInterface)\n",
    "\n",
    "---\n",
    "\n",
    "<!-- Notebook Header End -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚û° Importing Libraries & Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports\n",
    "from pathlib import Path\n",
    "import os\n",
    "from kilosort import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import math\n",
    "import seaborn as sns\n",
    "from patsy import dmatrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# custom imports\n",
    "from automations import RM1\n",
    "from automations import SpikeInterface_wrapper\n",
    "from automations import Kilosort_wrapper\n",
    "from automations import plots\n",
    "from automations import analysis_functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### probe definition\n",
    "\n",
    "Using the spreadsheet, `Adapter_pinout.xlsx`, the contact ID's can be traced to the \"device channel\", and we can assign them on the probe. \n",
    "\n",
    "In this case, our channel indices correspond to the aux inputs to the intan headstage.\n",
    "\n",
    "refer to the notebook, `RM1_pipeline.ipynb` within  the `dev_notebook` folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am running my latest analysis, but with Charlie's probe to make sure that I am doing this right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROBE_DIRECTORY = Path(r\"D:\\SynologyDrive\\CMU.80 Data\\88 Analyzed Data\\88.001 A1x32-Edge-5mm-20-177-A32\\A1x32-Edge-5mm-20-177-A32_max_murphy_probe.prb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filepath definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE Specify the path where the data will be copied to, and where Kilosort4 results will be saved.\n",
    "# in this case, the data is saved in a folder with multiple rats\n",
    "DATA_DIRECTORY = Path(fr'D:\\SynologyDrive\\CMU.80 Data\\82 External Data\\82.002 Sample Rat Data from RM1 Project')  \n",
    "# Create path if it doesn't exist\n",
    "DATA_DIRECTORY.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# NOTE Specify the path where the data will be copied to, and where Kilosort4 results will be saved.\n",
    "# save data to the inbox; make sure that the folders: binary & figures exist\n",
    "\n",
    "# select your path\n",
    "\n",
    "# the following save directory has already been run, and it stores data that has been unaltered\n",
    "SAVE_DIRECTORY = Path(fr\"D:\\SynologyDrive\\CMU.80 Data\\88 Analyzed Data\\88.016 Multi_Rat_Linear_Mixed_Effects_max_probe_noflex\")\n",
    "\n",
    "# Create paths if they don't exist\n",
    "SAVE_DIRECTORY.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run a multi-rat class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading DRGS_10_240918_140034...\n",
      "Error reading stream 2 for DRGS_10_240918_140034: stream_id 2 is not in ['0', '1', '3', '4']\n",
      "Reading DRGS_11_240918_140803...\n",
      "Error reading stream 2 for DRGS_11_240918_140803: stream_id 2 is not in ['0', '1', '3', '4']\n",
      "Reading DRGS_12_240918_141655...\n",
      "Error reading stream 2 for DRGS_12_240918_141655: stream_id 2 is not in ['0', '1', '3', '4']\n",
      "Reading DRGS_1_240918_125448...\n",
      "Error reading stream 2 for DRGS_1_240918_125448: stream_id 2 is not in ['0', '1', '3', '4']\n",
      "Reading DRGS_2_240918_130024...\n",
      "Error reading stream 2 for DRGS_2_240918_130024: stream_id 2 is not in ['0', '1', '3', '4']\n",
      "Reading DRGS_3_240918_130835...\n",
      "Error reading stream 2 for DRGS_3_240918_130835: stream_id 2 is not in ['0', '1', '3', '4']\n",
      "Reading DRGS_4_240918_131552...\n",
      "Error reading stream 2 for DRGS_4_240918_131552: stream_id 2 is not in ['0', '1', '3', '4']\n",
      "Reading DRGS_5_240918_132233...\n",
      "Error reading stream 2 for DRGS_5_240918_132233: stream_id 2 is not in ['0', '1', '3', '4']\n",
      "Reading DRGS_6_240918_132920...\n",
      "Error reading stream 2 for DRGS_6_240918_132920: stream_id 2 is not in ['0', '1', '3', '4']\n",
      "Reading DRGS_7_240918_133719...\n",
      "Error reading stream 2 for DRGS_7_240918_133719: stream_id 2 is not in ['0', '1', '3', '4']\n",
      "Reading DRGS_8_240918_134529...\n",
      "Error reading stream 2 for DRGS_8_240918_134529: stream_id 2 is not in ['0', '1', '3', '4']\n",
      "Reading DRGS_9_240918_135255...\n",
      "Error reading stream 2 for DRGS_9_240918_135255: stream_id 2 is not in ['0', '1', '3', '4']\n",
      "Reading VF_1_240918_143256...\n",
      "Error reading stream 2 for VF_1_240918_143256: stream_id 2 is not in ['0', '1', '3', '4']\n",
      "Reading VF_2_240918_143936...\n",
      "Error reading stream 2 for VF_2_240918_143936: stream_id 2 is not in ['0', '1', '3', '4']\n",
      "Reading VF_3_240918_144658...\n",
      "Error reading stream 2 for VF_3_240918_144658: stream_id 2 is not in ['0', '1', '3', '4']\n",
      "Reading VF_4_240918_145638...\n",
      "Error reading stream 2 for VF_4_240918_145638: stream_id 2 is not in ['0', '1', '3', '4']\n",
      "Reading VF_5_240918_150137...\n",
      "Error reading stream 2 for VF_5_240918_150137: stream_id 2 is not in ['0', '1', '3', '4']\n",
      "Reading VF_6_240918_150811...\n",
      "Error reading stream 2 for VF_6_240918_150811: stream_id 2 is not in ['0', '1', '3', '4']\n",
      "Reading VF_7_240918_151516...\n",
      "Error reading stream 2 for VF_7_240918_151516: stream_id 2 is not in ['0', '1', '3', '4']\n",
      "Reading VF_8_240918_152056...\n",
      "Error reading stream 2 for VF_8_240918_152056: stream_id 2 is not in ['0', '1', '3', '4']\n",
      "Reading VF_9_240918_152753...\n",
      "Error reading stream 2 for VF_9_240918_152753: stream_id 2 is not in ['0', '1', '3', '4']\n",
      "Reading DRGS_10_240911_155921...\n",
      "Error reading stream 2 for DRGS_10_240911_155921: stream_id 2 is not in ['0', '1', '3', '4']\n",
      "Reading DRGS_11_240911_160638...\n",
      "Error reading stream 2 for DRGS_11_240911_160638: stream_id 2 is not in ['0', '1', '3', '4']\n",
      "Reading DRGS_1_240911_144910...\n",
      "Error reading stream 2 for DRGS_1_240911_144910: stream_id 2 is not in ['0', '1', '3', '4']\n",
      "Reading DRGS_2_240911_145808...\n",
      "Error reading stream 2 for DRGS_2_240911_145808: stream_id 2 is not in ['0', '1', '3', '4']\n",
      "Reading DRGS_3_240911_150506...\n",
      "Error reading stream 2 for DRGS_3_240911_150506: stream_id 2 is not in ['0', '1', '3', '4']\n",
      "Reading DRGS_4_240911_151137...\n"
     ]
    }
   ],
   "source": [
    "# all trials\n",
    "rats = [RM1.Rat(DATA_DIRECTORY, PROBE_DIRECTORY, rat_id) for rat_id in ['DW322', 'DW323', 'DW327','DW361']]\n",
    "\n",
    "# just the one I want for now. you need to pass it as a list, even if it's just one\n",
    "# rats = [RM1.Rat(DATA_DIRECTORY, PROBE_DIRECTORY, rat_id) for rat_id in ['DW361']]\n",
    "\n",
    "group = RM1.RatGroup(rats)\n",
    "group.run_preprocessing(remove_drg_stim=True)  # preprocess all rats at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group.rats.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run spikeinterface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20250411 update: I am using the bandpass filter (300 Hz - 3000 Hz) from Ting et al. 2024 and the notch filter (60Hz) from Wang et al. 2024\n",
    "\n",
    "I am using 60 Hz instead of 50 Hz because Wang et al. 2024 was in China, and the supplied power is 50 Hz, and in the US it is 60 Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si_wrappers = group.create_spikeinterface_wrappers(SAVE_DIRECTORY)\n",
    "for rat_id in si_wrappers.keys():\n",
    "    # si_wrappers[rat_id].save_spinalcord_data_to_binary(bandpass_freq_min=300, bandpass_freq_max=3000, notch_freq=60)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run kilosort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_wrappers = group.create_kilosort_wrappers(SAVE_DIRECTORY, probe_directory=PROBE_DIRECTORY)\n",
    "def my_custom_criteria(cluster_labels, st, clu, est_contam_rate, fs):   \n",
    "    # Example criteria: Contamination rate < 15% and firing rate between 0.1 and 1000 Hz\n",
    "    contam_good = est_contam_rate < 0.15\n",
    "    fr_good = np.zeros(cluster_labels.size, dtype=bool)\n",
    "    for i, c in enumerate(cluster_labels):\n",
    "        spikes = st[clu == c]\n",
    "        fr = spikes.size / ((spikes.max() - spikes.min()) / fs)\n",
    "        if 0.1 <= fr <= 1000:\n",
    "            fr_good[i] = True\n",
    "    return np.logical_and(contam_good, fr_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rat_id in ks_wrappers.keys():\n",
    "    # ks_wrappers[rat_id].run_kilosort_trial_summary(new_settings=\"vf_settings\",custom_criteria=my_custom_criteria)\n",
    "    ks_wrappers[rat_id].extract_kilosort_outputs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate sanity plots for each trial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw322_trials = [\"VF_1_240918_143256\",\"VF_2_240918_143936\",\"VF_3_240918_144658\",\"VF_4_240918_145638\",\"VF_5_240918_150137\",\"VF_6_240918_150811\",\n",
    "                \"VF_7_240918_151516\",\"VF_8_240918_152056\",\"VF_9_240918_152753\"]\n",
    "\n",
    "dw323_trials = [\"VF_1_240911_164342\",\"VF_2_240911_165039\",\"VF_3_240911_165617\",\"VF_4_240911_170446\",\"VF_5_240911_171014\",\"VF_6_240911_171505\"]\n",
    "\n",
    "dw327_trials = [\"VF_01_241125_153746\",\"VF_02_241125_154307\",\"VF_03_241125_154841\",\"VF_04_241125_155417\",\"VF_05_241125_155941\",\"VF_06_241125_160515\",\n",
    "                \"VF_07_241125_161126\",\"VF_08_241125_161626\",\"VF_09_241125_162141\",\"VF_10_241125_162725\"]\n",
    "\n",
    "dw361_trials = [\"VF_1_250422_124925\",\"VF_2_250422_125146\",\"VF_3_250422_125632\",\"VF_4_250422_130213\",\"VF_5_250422_130729\",\"VF_6_250422_131240\",\n",
    "                \"VF_7_250422_131739\",\"VF_8_250422_132231\",\"VF_9_250422_132717\",\"VF_10_250422_133218\",\"VF_11_250422_133722\",\"VF_12_250422_134214\",\n",
    "                \"VF_13_250422_134725\",\"VF_14_250422_154410\",\"VF_14_250422_154410\",\"VF_15_250422_154935\",\"VF_16_250422_155509\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NOTE:\n",
    "\n",
    "seeing repeating units is not a sign that there is an issue: these plots a pick random units, (40) times, to show how spikes are captured on the probe. \n",
    "\n",
    "the fewer the number of good or mua units, the more likely there will be repeated units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DW322"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ks_wrappers[\"DW322\"].plot_trial_results(dw322_trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DW323"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ks_wrappers[\"DW323\"].plot_trial_results(dw323_trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DW327"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ks_wrappers[\"DW327\"].plot_trial_results(dw327_trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DW361"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ks_wrappers[\"DW361\"].plot_trial_results(dw361_trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot characteristic waveform for each neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DW322"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ks_wrappers[\"DW322\"].plot_cluster_waveforms(dw322_trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DW323"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ks_wrappers[\"DW323\"].plot_cluster_waveforms(dw323_trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DW327"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ks_wrappers[\"DW327\"].plot_cluster_waveforms(dw327_trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DW361"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ks_wrappers[\"DW361\"].plot_cluster_waveforms(dw361_trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multi-animal von frey analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a multi‚Äìrat analysis instance.\n",
    "multi_analysis = analysis_functions.MultiRatVonFreyAnalysis(group, si_wrappers, ks_wrappers)\n",
    "\n",
    "# combine all the trials from every animal to plot the results\n",
    "\n",
    "combined_results = multi_analysis.analyze_all_trials(excel_parent_folder=SAVE_DIRECTORY, subwindow_width=0.5, corr_threshold=0.01)\n",
    "\n",
    "# use combined_results for further plotting or modeling.\n",
    "\n",
    "# combined_results now contains results from each rat, loaded from Excel if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_results[\"DW322_VF_2_240918_143936\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the Results and Metadata:\n",
    "You would need to create (or extract) a DataFrame where each row corresponds to a sub-window (or trial) with columns for the dependent variable (for example, average voltage), the fixed effects (e.g. stimulation, pulse width, waiting), and a column indicating the animal (rat ID). You might need to merge your VonFreyAnalysis results with metadata that specifies stimulation parameters.\n",
    "\n",
    "Fit a Mixed Effects Model:\n",
    "Use the statsmodels MixedLM or the formula interface (statsmodels.formula.api.mixedlm) to specify a model with fixed effects for voltage, stimulation, pulse width, waiting and a random intercept for animal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example of single-rat experiment properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group.rats[\"DW323\"].qst_trial_notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combining the experiment notes for individual animals, for combined plots and linear mixed effects modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_unique_cols(df):\n",
    "    new_cols = []\n",
    "    seen = {}\n",
    "    for col in df.columns:\n",
    "        if col in seen:\n",
    "            seen[col] += 1\n",
    "            new_cols.append(f\"{col}_{seen[col]}\")\n",
    "        else:\n",
    "            seen[col] = 0\n",
    "            new_cols.append(col)\n",
    "    df.columns = new_cols\n",
    "    return df\n",
    "\n",
    "dfs = []\n",
    "for rat_id, rat in group.rats.items():\n",
    "    df = make_unique_cols(rat.qst_trial_notes.copy())\n",
    "    df['Rat ID'] = rat_id\n",
    "    df[\"Trial Number\"] = rat_id + \"_\" + df[\"Trial Number\"].astype(str)\n",
    "    dfs.append(df)\n",
    "\n",
    "combined_qst_notes = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Sort keys based on the numeric part after the second underscore (i.e., the zero-padded trial number)\n",
    "sorted_keys = sorted(\n",
    "    combined_results.keys(),\n",
    "    key=lambda x: int(x.split('_')[2])\n",
    ")\n",
    "\n",
    "combined_qst_notes['Trial_ID'] = sorted_keys\n",
    "\n",
    "# After concatenating your DataFrames into combined_qst_notes:\n",
    "combined_qst_notes['Trial_ID'] = list(combined_results.keys())\n",
    "combined_qst_notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot a composite Von Frey Analysis plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example:\n",
    "plots.vf_all_trials_combined_plot(combined_results, combined_qst_notes,corr_threshold=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broke the code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## combine metadata and neuron firing changes for linear effects mixed model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rev 1 - broken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def flatten_and_filter_by_corr(combined_results, combined_qst_notes, corr_threshold=0.1):\n",
    "#     \"\"\"\n",
    "#     Flattens trial data into a single DataFrame, then filters clusters by correlation threshold.\n",
    "#     Only clusters whose absolute correlation meets or exceeds corr_threshold are kept.\n",
    "#     \"\"\"\n",
    "#     # Create dictionaries for columns you need from combined_qst_notes\n",
    "#     freq_dict = dict(zip(combined_qst_notes['Trial_ID'], combined_qst_notes['Freq. (Hz)']))\n",
    "#     amp_dict = dict(zip(combined_qst_notes['Trial_ID'], combined_qst_notes['amp']))\n",
    "#     # Add a rat_id_dict to capture the rat ID from combined_qst_notes\n",
    "#     rat_id_dict = dict(zip(combined_qst_notes['Trial_ID'], combined_qst_notes['Rat ID']))\n",
    "#     pulse_width_dict = dict(zip(combined_qst_notes['Trial_ID'], combined_qst_notes['PW (us)']))\n",
    "\n",
    "\n",
    "\n",
    "#     records = []\n",
    "\n",
    "#     for trial_id, res in combined_results.items():\n",
    "#         # Get frequency and amplitude\n",
    "#         freq_hz = freq_dict.get(trial_id)\n",
    "#         amp_val = amp_dict.get(trial_id)\n",
    "#         rat_id = rat_id_dict.get(trial_id)\n",
    "#         pulse_width = pulse_width_dict.get(trial_id)\n",
    "\n",
    "#         if freq_hz is None or amp_val is None or rat_id is None or pulse_width is None:\n",
    "#             continue\n",
    "\n",
    "#         avg_voltage_df = res.get('voltage_df')\n",
    "#         firing_rates_df = res.get('firing_df')\n",
    "#         if avg_voltage_df is None or firing_rates_df is None:\n",
    "#             continue\n",
    "        \n",
    "#         # Skip if not enough rows to have correlation data\n",
    "#         if len(firing_rates_df) < 2:\n",
    "#             continue\n",
    "\n",
    "#         # Extract the correlation row (second-to-last row)\n",
    "#         correlation_data = firing_rates_df.iloc[-2]\n",
    "#         # Convert correlation entries to a dictionary keyed by integer cluster IDs\n",
    "#         corr_dict = {\n",
    "#             int(k): v\n",
    "#             for k, v in correlation_data.to_dict().items()\n",
    "#             if str(k).isdigit()\n",
    "#         }\n",
    "\n",
    "#         # Exclude the last two rows (correlation row, plus any extra row if present)\n",
    "#         firing_data = firing_rates_df.iloc[:-2]\n",
    "\n",
    "#         # Melt the firing data to long format: (group, cluster, firing_rate)\n",
    "#         firing_melt = firing_data.melt(\n",
    "#             id_vars=['group'], \n",
    "#             var_name='cluster', \n",
    "#             value_name='firing_rate'\n",
    "#         )\n",
    "#         firing_melt['cluster'] = pd.to_numeric(firing_melt['cluster'], errors='coerce')\n",
    "\n",
    "#         # Merge firing data with average voltage on group\n",
    "#         if \"avg_voltage\" not in avg_voltage_df.columns:\n",
    "#             continue\n",
    "#         merged_df = pd.merge(\n",
    "#             firing_melt,\n",
    "#             avg_voltage_df[[\"group\", \"avg_voltage\"]],\n",
    "#             on=\"group\",\n",
    "#             how=\"left\"\n",
    "#         )\n",
    "\n",
    "#         # Map correlation values per cluster\n",
    "#         merged_df['correlation'] = merged_df['cluster'].map(corr_dict)\n",
    "\n",
    "#         # Add identifying columns\n",
    "#         merged_df['Trial_ID'] = trial_id\n",
    "#         merged_df['Frequency_Hz'] = freq_hz\n",
    "#         merged_df['Amplitude'] = amp_val\n",
    "#         merged_df[\"Pulse_Width(us)\"] = pulse_width\n",
    "#         # normalize the results by dividing by average voltage\n",
    "#         merged_df[\"firing_rate_vs_stim(uV^-1)\"] = merged_df['Frequency_Hz'] / merged_df[\"avg_voltage\"]\n",
    "#         merged_df['Rat_ID'] = rat_id\n",
    "\n",
    "#         # if I need to further flatten the dataset\n",
    "#         merged_df['cluster'] = merged_df['cluster'].astype(str) + \"-\" + merged_df[\"Trial_ID\"].astype(str)\n",
    "        \n",
    "#         # Collect all in a list\n",
    "#         records.append(merged_df)\n",
    "\n",
    "#     # Combine all into a single DataFrame\n",
    "#     if not records:\n",
    "#         return pd.DataFrame()\n",
    "\n",
    "#     final_long_df = pd.concat(records, ignore_index=True)\n",
    "\n",
    "#     # Filter by correlation threshold. Keep only those meeting abs corr >= threshold\n",
    "#     final_long_df = final_long_df[final_long_df['correlation'].abs() >= corr_threshold]\n",
    "\n",
    "#     return final_long_df\n",
    "\n",
    "\n",
    "# lmem = flatten_and_filter_by_corr(combined_results, combined_qst_notes, corr_threshold=0.01)\n",
    "# lmem.to_csv(SAVE_DIRECTORY / \"flattened_data.csv\")\n",
    "# lmem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rev2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_and_filter_by_corr(combined_results,\n",
    "                               combined_qst_notes,\n",
    "                               corr_threshold=0.01):\n",
    "    \"\"\"\n",
    "    For each trial in combined_results:\n",
    "      ‚Ä¢ extract per-neuron correlation (last two rows of firing_df)\n",
    "      ‚Ä¢ keep only clusters whose |corr| >= corr_threshold\n",
    "      ‚Ä¢ melt firing rates, tag with timepoint index\n",
    "      ‚Ä¢ tag voltage with the same timepoint index\n",
    "      ‚Ä¢ merge one-to-one on timepoint\n",
    "      ‚Ä¢ attach trial metadata from combined_qst_notes\n",
    "    Returns a single concatenated DataFrame.\n",
    "    \"\"\"\n",
    "    # build lookups from your QST notes\n",
    "    meta = combined_qst_notes.set_index('Trial_ID')\n",
    "    records = []\n",
    "\n",
    "    for trial_id, res in combined_results.items():\n",
    "        if trial_id not in meta.index:\n",
    "            continue\n",
    "        # pull trial metadata\n",
    "        freq_hz    = meta.at[trial_id, 'Freq. (Hz)']\n",
    "        amp_val    = meta.at[trial_id, 'amp']\n",
    "        pw_us      = meta.at[trial_id, 'PW (us)']\n",
    "        rat_id     = meta.at[trial_id, 'Rat ID']\n",
    "\n",
    "        vdf = res.get('voltage_df')\n",
    "        fdf = res.get('firing_df')\n",
    "        if vdf is None or fdf is None or len(fdf) < 2:\n",
    "            continue\n",
    "\n",
    "        # 1) extract correlations (second-to-last row), build corr_dict\n",
    "        corr_row = fdf.iloc[-2].to_dict()\n",
    "        corr_dict = {int(k): float(v)\n",
    "                     for k, v in corr_row.items()\n",
    "                     if isinstance(k, (str, int)) and str(k).isdigit()}\n",
    "\n",
    "        # 2) pick only clusters passing threshold\n",
    "        valid_clusters = {c for c, r in corr_dict.items()\n",
    "                          if abs(r) >= corr_threshold}\n",
    "        if not valid_clusters:\n",
    "            continue\n",
    "\n",
    "        # 3) clean off the two bottom rows, tag each row with its original index\n",
    "        firing_clean = fdf.iloc[:-2].copy()\n",
    "        firing_clean['timepoint'] = firing_clean.index\n",
    "\n",
    "        voltage_clean = vdf.copy()\n",
    "        voltage_clean['timepoint'] = voltage_clean.index\n",
    "\n",
    "        # 4) melt firing ‚Üí long, convert cluster ‚Üí numeric, filter by valid_clusters\n",
    "        fm = (\n",
    "            firing_clean\n",
    "            .melt(\n",
    "                id_vars=['timepoint', 'group'],\n",
    "                var_name='cluster',\n",
    "                value_name='firing_rate'\n",
    "            )\n",
    "            .assign(cluster=lambda df: pd.to_numeric(df['cluster'], errors='coerce'))\n",
    "            .query(\"cluster in @valid_clusters\")\n",
    "        )\n",
    "\n",
    "        # 5) one-to-one merge on timepoint (group could be included if you want)\n",
    "        merged = (\n",
    "            fm\n",
    "            .merge(\n",
    "                voltage_clean[['timepoint', 'avg_voltage', 'group']],\n",
    "                on=['timepoint', 'group'],\n",
    "                how='left'\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # 6) attach correlation & trial metadata\n",
    "        merged['correlation']              = merged['cluster'].map(corr_dict)\n",
    "        merged['Trial_ID']                 = trial_id\n",
    "        merged['Frequency_Hz']             = freq_hz\n",
    "        merged['Amplitude']                = amp_val\n",
    "        merged['Pulse_Width(us)']          = pw_us\n",
    "        merged['Rat_ID']                   = rat_id\n",
    "        merged['firing_rate_vs_stim(uV^-1)'] = merged['Frequency_Hz'] / merged['avg_voltage']\n",
    "\n",
    "        records.append(merged)\n",
    "\n",
    "    # concatenate all trials\n",
    "    if not records:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    final = pd.concat(records, ignore_index=True)\n",
    "    return final\n",
    "\n",
    "\n",
    "lmem = flatten_and_filter_by_corr(combined_results,\n",
    "                                  combined_qst_notes,\n",
    "                                  corr_threshold=0.01)\n",
    "lmem.to_csv(SAVE_DIRECTORY / \"flattened_data_rev2.csv\", index=False)\n",
    "lmem.iloc[1:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting a better look at the force distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# histogram\n",
    "\n",
    "pre_data = lmem[lmem['group'] == 'pre-stim']['avg_voltage']\n",
    "post_data = lmem[lmem['group'] == 'post-stim']['avg_voltage']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(pre_data, bins=30, alpha=0.5, label='pre-stim', edgecolor='black')\n",
    "plt.hist(post_data, bins=30, alpha=0.5, label='post-stim', edgecolor='black')\n",
    "plt.title('Histogram of Average Voltage: pre vs post stim')\n",
    "plt.xlabel('Average Voltage')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# boxplot\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "data_to_plot = [\n",
    "    pre_data,\n",
    "    post_data\n",
    "]\n",
    "plt.boxplot(data_to_plot, labels=['pre-stim', 'post-stim'])\n",
    "plt.title('Box Plot of Average Voltage: pre vs post stim')\n",
    "plt.ylabel('Average Voltage')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it honestly doesn't look too different - what could the issue be??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grouping the dataset by \"bins\" of stimulus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Define bin edges: we go from 0 to slightly above the max value in steps of 50,000.\n",
    "max_val = lmem['avg_voltage'].max()\n",
    "bin_edges = np.arange(0, max_val + 10000, 10000)\n",
    "\n",
    "# 2. Create labels that match the upper bound of each bin.\n",
    "#    For instance, bin [0, 50000] gets the label \"50000\", bin (50000, 100000] gets \"100000\", etc.\n",
    "labels = [str(int(x)) for x in bin_edges[1:]]\n",
    "\n",
    "# 3. Use pd.cut to discretize avg_voltage. \n",
    "#    include_lowest=True ensures that 0 goes into the first bin. \n",
    "#    With right=True (the default), intervals are (a, b], so 50,000 falls in the first bin.\n",
    "lmem['voltage_bin'] = pd.cut(lmem['avg_voltage'], \n",
    "                           bins=bin_edges, \n",
    "                           labels=labels, \n",
    "                           include_lowest=True,\n",
    "                           right=True)\n",
    "\n",
    "# Now each row has a \"voltage_bin\" indicating its bin.\n",
    "\n",
    "\n",
    "# see bin counts\n",
    "bin_counts = lmem['voltage_bin'].value_counts().sort_index()\n",
    "print(\"Number of entries in each bin (uV):\")\n",
    "print(bin_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'cluster' and 'voltage_bin' then calc mean firing rate\n",
    "avg_firing_rate = lmem.groupby(['Rat_ID',\"Trial_ID\",'cluster', 'voltage_bin', \"Frequency_Hz\", 'group'])['firing_rate'].mean().reset_index().dropna()\n",
    "# avg_firing_rate[\"firing_rate\"] = avg_firing_rate[\"firing_rate\"].astype(float) # I think it already does this, no need to specify againa\n",
    "avg_firing_rate.to_csv(SAVE_DIRECTORY / \"grouped_stimulus.csv\")\n",
    "avg_firing_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT: this code need to drop zero entries, because you can't calculate % change with zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the data so that pre-stim and post-stim firing rates become separate columns.\n",
    "pre_post_pivot = avg_firing_rate.pivot_table(index=['Rat_ID', 'Trial_ID', 'cluster', 'voltage_bin', 'Frequency_Hz'],\n",
    "                          columns='group',\n",
    "                          values='firing_rate').reset_index()\n",
    "\n",
    "\n",
    "# Remove rows where pre-stim is zero to avoid division by zero\n",
    "pre_post_pivot = pre_post_pivot[pre_post_pivot['pre-stim'] != 0]\n",
    "\n",
    "\n",
    "# Drop rows that do not have both pre-stim and post-stim values\n",
    "pre_post_pivot = pre_post_pivot.dropna(subset=['pre-stim', 'post-stim'])\n",
    "\n",
    "# Calc percent change, pre to post\n",
    "pre_post_pivot['percent_change'] = ((pre_post_pivot['post-stim'] - pre_post_pivot['pre-stim']) / pre_post_pivot['pre-stim']) * 100\n",
    "\n",
    "pre_post_pivot.to_csv(SAVE_DIRECTORY / \"percent_change.csv\")\n",
    "\n",
    "pre_post_pivot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### converting the pivoted data to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## voltage bin\n",
    "# convert category ‚Üí string ‚Üí float (invalid entries become NaN)\n",
    "pre_post_pivot[\"voltage_bin\"] = pd.to_numeric(\n",
    "    pre_post_pivot[\"voltage_bin\"].astype(str), errors=\"coerce\"\n",
    ")\n",
    "\n",
    "## pre-post stim\n",
    "# turn everything numeric, coerce errors ‚Üí NaN\n",
    "for col in [\"pre-stim\", \"post-stim\"]:\n",
    "    pre_post_pivot[col] = pd.to_numeric(pre_post_pivot[col], errors=\"coerce\")\n",
    "\n",
    "# drop rows that failed conversion or have zero pre‚Äëstim\n",
    "pre_post_pivot = pre_post_pivot.dropna(subset=[\"pre-stim\", \"post-stim\"])\n",
    "pre_post_pivot = pre_post_pivot[pre_post_pivot[\"pre-stim\"] != 0]\n",
    "\n",
    "## percent change\n",
    "# recompute percent_change as float\n",
    "pre_post_pivot[\"percent_change\"] = (\n",
    "    (pre_post_pivot[\"post-stim\"] - pre_post_pivot[\"pre-stim\"])\n",
    "    / pre_post_pivot[\"pre-stim\"]\n",
    ") * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pre_post_pivot.dtypes.loc[[\"voltage_bin\", \"pre-stim\",\n",
    "                                 \"post-stim\", \"percent_change\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä % Change in Firing Rates, Grouped by Force (force-matched plots)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea:\n",
    "* The goal here is to create a plot similar to what I already have. But what I will do is plot the percent change in firing rate, for each neuron, along a batch of stimulation rates. so essentially, break up the voltage thresholds into a batch of voltage ranges, then group the firing rates in those batches, then plot the difference.\n",
    "\n",
    "## Steps\n",
    "1) Break up the voltages into batches of 100,000uV (there should be 4-8 groups within this range)\n",
    "2) For each neuron, calculate the percent change in firing rate for each neuron, pre & post stimulation\n",
    "3) plot the percent change on the y axis. do this for every neuron in the group. \n",
    "4) plot the percent change for each neuron to the color representing it's DRG stimulation frequency (5Hz, 20Hz, 100Hz)\n",
    "5) draw a trendline for the percent change for each neuron (make the colors very similar to what I was using for the previous plot)\n",
    "\n",
    "the percent change is given by:\n",
    "\n",
    "$\\%\\ Change, Firing\\ Rate = \\frac{((Post-Stimulation\\ Firing\\ Rate))-(Pre-Stimulation\\ Firing\\ Rate)}{Pre-Stimulation\\ Firing\\ Rate}$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_post_pivot.sort_values([\"percent_change\"], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_percent_change(\n",
    "    df: pd.DataFrame,\n",
    "    voltage_bin_col: str = \"voltage_bin\",\n",
    "    pre_col: str = \"pre-stim\",\n",
    "    post_col: str = \"post-stim\",\n",
    "    freq_col: str = \"Frequency_Hz\",\n",
    "    percent_col: str = \"percent_change\",\n",
    "    cluster_col: str = \"cluster\",\n",
    "    add_trendlines: bool = True,\n",
    "    cmap_name: str = \"tab10\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Scatter‚Äëplot percent change in firing rate versus voltage bin midpoint.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Combined dataframe across all rats and trials.\n",
    "    voltage_bin_col, pre_col, post_col, freq_col, percent_col, cluster_col : str\n",
    "        Column names inside `df`.\n",
    "    add_trendlines : bool\n",
    "        Draw a linear trendline per frequency group when True.\n",
    "    cmap_name : str\n",
    "        Matplotlib colormap for assigning colors to frequency groups.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Basic column check ---------------------------------------------------\n",
    "    needed = {voltage_bin_col, pre_col, post_col, freq_col, cluster_col}\n",
    "    missing = needed - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Dataframe missing columns: {missing}\")\n",
    "    \n",
    "\n",
    "    # --- force voltage_bin to numeric even if categorical or object ---\n",
    "    df = df.copy()\n",
    "    df[voltage_bin_col] = pd.to_numeric(df[voltage_bin_col].astype(str), errors=\"coerce\")\n",
    "    df = df.dropna(subset=[voltage_bin_col])        # remove rows that failed coercion\n",
    "\n",
    "    # --- Compute percent_change if absent ------------------------------------\n",
    "    if percent_col not in df.columns:\n",
    "        # avoid divide‚Äëby‚Äëzero and NaNs\n",
    "        clean = df.dropna(subset=[pre_col, post_col]).copy()\n",
    "        clean = clean[clean[pre_col] != 0]\n",
    "        df[percent_col] = ((clean[post_col] - clean[pre_col]) / clean[pre_col]) * 100\n",
    "\n",
    "    # --- Remove any rows still lacking valid percent_change ------------------\n",
    "    plot_df = df.dropna(subset=[percent_col, voltage_bin_col, freq_col])\n",
    "\n",
    "    if plot_df.empty:\n",
    "        print(\"No data available after cleaning.\")\n",
    "        return\n",
    "\n",
    "    # --- Color mapping -------------------------------------------------------\n",
    "    freqs = np.sort(plot_df[freq_col].unique())\n",
    "    cmap = plt.get_cmap(cmap_name)\n",
    "    color_map = {f: cmap(i % cmap.N) for i, f in enumerate(freqs)}\n",
    "\n",
    "    # --- Plot ----------------------------------------------------------------\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = plt.gca()\n",
    "\n",
    "    for f in freqs:\n",
    "        subset = plot_df[plot_df[freq_col] == f]\n",
    "        x = subset[voltage_bin_col].values\n",
    "        y = subset[percent_col].values\n",
    "\n",
    "        ax.scatter(x, y, color=color_map[f], alpha=0.7, label=f\"{f}¬†Hz\")\n",
    "\n",
    "        # trendline only if at least two distinct x values\n",
    "        if add_trendlines and np.unique(x).size > 1:\n",
    "            slope, intercept = np.polyfit(x, y, 1)\n",
    "            x_line = np.linspace(x.min(), x.max(), 100)\n",
    "            y_line = slope * x_line + intercept\n",
    "            ax.plot(x_line, y_line, color=color_map[f], linewidth=2, alpha=0.8)\n",
    "\n",
    "    # --- Decorations ---------------------------------------------------------\n",
    "    ax.axhline(0, color=\"k\", linestyle=\"--\", linewidth=1)\n",
    "    ax.set_xlabel(\"Voltage Bin Midpoint (¬µV)\")\n",
    "    ax.set_ylabel(\"Percent Change in Firing Rate (%)\")\n",
    "    ax.set_title(\"Percent Change in Firing Rate by Voltage Bin\")\n",
    "    ax.legend(title=\"DRG Stimulation Frequency\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all data, all rats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_percent_change(pre_post_pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DW323"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_post_pivot[pre_post_pivot[\"Rat_ID\"]==\"DW323\"].sort_values([\"percent_change\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_percent_change(pre_post_pivot[pre_post_pivot[\"Rat_ID\"]==\"DW323\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DW327"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_post_pivot[pre_post_pivot[\"Rat_ID\"]==\"DW327\"].sort_values([\"percent_change\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_post_pivot[pre_post_pivot[\"Rat_ID\"]==\"DW327\"].sort_values([\"percent_change\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_percent_change(pre_post_pivot[pre_post_pivot[\"Rat_ID\"]==\"DW327\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DW322"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_post_pivot[pre_post_pivot[\"Rat_ID\"]==\"DW322\"].sort_values([\"percent_change\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_percent_change(pre_post_pivot[pre_post_pivot[\"Rat_ID\"]==\"DW322\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìà linear mixed effects model - nested cluster, trial, rat id\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAKE SURE TO DO THIS STEP\n",
    "\n",
    "\n",
    "when you filter by correlation threshhold, it is going to drop rows and some of the data may refer to old indices.  This can happen if you drop rows (for example, due to missing percent_change values) without resetting the index, so some groups still reference old index values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this graph is hard to draw any conclusions from - plot the average trend, with std error bars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    #### structure of dataset\n",
    "\n",
    "    * Rat\n",
    "        * trial \n",
    "            * cluster ID\n",
    "\n",
    "    #### Predictor/Fixed Variable:\n",
    "    * Frequency_Hz (categorical: 5, 20, 100 Hz): frequency of DRG stimulation\n",
    "    #### Random Effects:\n",
    "    * Rat_ID: Differences in baseline firing changes between animals.\n",
    "    * Trial_ID (nested within Rat_ID): Trial‚Äêto‚Äêtrial variability (per animal).\n",
    "    * Trial_Cluster: Variability at the neuron level (per trial)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Formula:**\n",
    "\n",
    "$$\n",
    "\\text{percent\\_change\\_firing} \\sim \\text{Frequency\\_Hz} + (1 \\mid \\text{Rat\\_ID}) + \\{ \\text{Trial}: 0 + C(\\text{Trial\\_ID}),\\ \\text{Neuron}: 0 + C(\\text{Cluster}) \\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert variables to categorical types\n",
    "pre_post_pivot['Rat_ID']   = pre_post_pivot['Rat_ID'].astype('category')\n",
    "pre_post_pivot['Trial_ID'] = pre_post_pivot['Trial_ID'].astype('category')\n",
    "pre_post_pivot['cluster']  = pre_post_pivot['cluster'].astype('category')\n",
    "pre_post_pivot[\"percent_change\"] = pre_post_pivot[\"percent_change\"].astype('float')\n",
    "\n",
    "\n",
    "# Define variance components for trial and neuron (cluster)\n",
    "vc = {\n",
    "    'Trial': '0 + C(Trial_ID)',\n",
    "    'Neuron': '0 + C(cluster)',\n",
    "}\n",
    "\n",
    "# Specify the mixed effects model\n",
    "model = smf.mixedlm(\"percent_change ~ Frequency_Hz + voltage_bin\", pre_post_pivot, groups=pre_post_pivot[\"Rat_ID\"], vc_formula=vc)\n",
    "result = model.fit()\n",
    "\n",
    "# Display the model summary\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "things that could be possible:\n",
    "\n",
    "1. Multicollinearity - are Frequency and Voltage correlated?\n",
    "2. Trial_ID cluster have only than one level per group.\n",
    "3. Trial_ID cluster levels are not overly sparse or redundant with the fixed effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_post_pivot['Frequency_Hz'].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_post_pivot['voltage_bin'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_post_pivot[['Frequency_Hz', 'voltage_bin']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok, so these values are not highly correlated. this tells me we don't have enough data then?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert variables to categorical types\n",
    "pre_post_pivot['Rat_ID']   = pre_post_pivot['Rat_ID'].astype('category')\n",
    "pre_post_pivot['Trial_ID'] = pre_post_pivot['Trial_ID'].astype('category')\n",
    "pre_post_pivot['cluster']  = pre_post_pivot['cluster'].astype('category')\n",
    "pre_post_pivot[\"percent_change\"] = pre_post_pivot[\"percent_change\"].astype('float')\n",
    "\n",
    "\n",
    "# Define variance components for trial and neuron (cluster)\n",
    "vc = {\n",
    "    'Trial': '0 + C(Trial_ID)',\n",
    "    'Neuron': '0 + C(Trial_ID):C(cluster)'  # neurons nested within trials\n",
    "}\n",
    "\n",
    "# Specify the mixed effects model\n",
    "model = smf.mixedlm(\"percent_change ~ Frequency_Hz\", pre_post_pivot, groups=pre_post_pivot[\"Rat_ID\"], vc_formula=vc)\n",
    "result = model.fit()\n",
    "\n",
    "# Display the model summary\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert variables to categorical types\n",
    "pre_post_pivot['Rat_ID']   = pre_post_pivot['Rat_ID'].astype('category')\n",
    "pre_post_pivot['Trial_ID'] = pre_post_pivot['Trial_ID'].astype('category')\n",
    "pre_post_pivot['cluster']  = pre_post_pivot['cluster'].astype('category')\n",
    "pre_post_pivot[\"percent_change\"] = pre_post_pivot[\"percent_change\"].astype('float')\n",
    "\n",
    "\n",
    "# Define variance components for trial and neuron (cluster)\n",
    "vc = {\n",
    "    'Neuron': 'C(cluster)'  # neurons nested within trials\n",
    "}\n",
    "\n",
    "# Specify the mixed effects model\n",
    "model = smf.mixedlm(\"percent_change ~ Frequency_Hz\", pre_post_pivot, groups=pre_post_pivot[\"Trial_ID\"], vc_formula=vc)\n",
    "result = model.fit()\n",
    "\n",
    "# Display the model summary\n",
    "print(result.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
